Simple Metaprogramming
======================

Reflection in terms of Smalltalks `#perform`:, i.e., reflective method
invocation and the `#doesNotUnderstand:` handler for methods that are not
implemented by a given object.


```{r init, echo=FALSE, message=FALSE, warning=FALSE, results='hide', errors=FALSE}
# load libraries, the data, and prepare it
if (Sys.getenv("RSTUDIO") == "1") { setwd("/Users/smarr/Projects/PostDoc/ZERO-MOP/perf-eval-selfopt/") }

options(scipen=5)

source("scripts/libraries.R", chdir=TRUE)
data <- load_data_file("data/zero-overhead.data")
data <- subset(data, select = c(Value, Unit, Benchmark, VM, Suite, Var, rid))
data <- prepare_vm_names(data)

## Add a time series id
data <- ddply(data, ~ Benchmark + VM + Var, transform,
              Iteration = rid - min(rid))

stats <- ddply(data, ~ Benchmark + VM + Suite + Var,
               summarise,
               Time.geomean              = geometric.mean(Value),
               Time.stddev               = sd(Value),
               Time.median               = median(Value),
               max = max(Value),
               min = min(Value),
              ci.upper = CI(Value, ci = 0.99)['upper'],
              ci.lower=  CI(Value, ci = 0.99)['lower'],
              ci.mean =  CI(Value, ci = 0.99)['mean'])
#stats

# t <- droplevels(subset(data, Suite == "reflection" & VM == "RTruffleSOM" & Iteration > 50 & Iteration < 100))
# ddply(t, ~ Benchmark + VM + Suite,
#                summarise,
#                Time.geomean              = geometric.mean(Value),
#                Time.stddev               = sd(Value),
#                Time.median               = median(Value),
#                max = max(Value),
#                min = min(Value),
#               ci.upper = CI(Value, ci = 0.999999)['upper'],
#               ci.lower=  CI(Value, ci = 0.999999)['lower'],
#               ci.mean =  CI(Value, ci = 0.999999)['mean'])
```

The reflection benchmarks were execute on a number of different VMs.
TODO: move discussion to methodology section, about the graal splitting.
And remove also from other subsections.

```{r boxplots, echo=FALSE}
refl <- subset(data, Suite == "reflection" & Iteration > 50)
refl <- droplevels(refl)

#summary(refl)
#refl <- subset(data, Suite == "reflection" & Extra != "150 0 100")
#refl <- subset(refl, select = c(Value, Unit, Benchmark, VM, Suite, rid))
#refl <- prepare_vm_names(refl)

## Add a time series id
#refl <- ddply(refl, ~ Benchmark + VM, transform,
#              Iteration = rid - min(rid))

#



for (vm in levels(refl$VM)) {
  #vm <- "TruffleSOM.os"
  r <- subset(refl, VM == vm)
  r <- droplevels(r)
  
  p <- ggplot(r, aes(x=Benchmark, y=Value))
  p <- p + ggtitle(vm)
  print(p + geom_boxplot())
}

# subset(stats, VM == "RTruffleSOM" & Suite == "reflection")
# 
# vms <- levels(refl$VM)
# for (bench_name in levels(refl$Benchmark)) {
# for (vm in vms) {
#   #bench_name = "Bounce"
#   b <- droplevels(subset(refl, Benchmark == bench_name & VM == vm))
#   
#   upperBound <- 2 * median(b$Value)
#   b <- ddply(b, ~ Benchmark + Var + VM, transform,
#              ValCut = pmin(Value, upperBound))
#   # b$Foo <- as.factor(paste0(as.character(b$VM), '-', as.character(b$Var)))
#     
#   #plot <- ggplot(b, aes(x=Iteration, y=ValCut)) + geom_line(aes(colour = Var)) #+ facet_grid(VM + Benchmark ~ .)
#   plot <- ggplot(b, aes(x=Iteration, y=ValCut)) + geom_line(aes(colour = interaction(Var, VM))) #+ facet_grid(VM + Benchmark ~ .)
#   plot <- plot + ggtitle(paste0(bench_name, ' - ', vm))
#     #plot <- plot + scale_y_continuous(limits = c(0, 3000))
#   print(plot)
#     
#     # file_name = paste0("cut-timeline-", bench_name, ".pdf")
#     # cat(file_name)
#     # ggsave(plot, file=file_name)
# }}
```



Since these graphs are showing that the medians are not identical, but on an
absolute scale very close, we investigate further the generated traces and
native code for the benchmarks.

TODO: add links to data files and a little explanation what to see where.
Perhaps also with snippets taken from the files.

