# Analysis of the Actor Performance

The last complete run of this benchmark setup yielded the results presented
below. This report was generated on `r Sys.time()`.

```{r load-scripts, echo=FALSE, include=FALSE}
# load libraries, the data, and prepare it
if (Sys.getenv("RSTUDIO") == "1") { setwd("/Users/smarr/Projects/PostDoc/FASTXX/are-we-fast-yet/report") }
source("scripts/libraries.R", chdir=TRUE)
opts_chunk$set(dev = 'png',
               dev.args=list(pointsize=10),
               echo = FALSE,
               fig.keep='all',
               fig.path="figures/",
               external=FALSE,
               tidy=FALSE)
#    cache=TRUE,

vm_names <- c(
  "SOMns"                 = "SOMns core",
  "SOMns-Enterprise"      = "SOMns GraalVM",
  
  "AkkaActor"    = "Akka",
  "JetlangActor" = "Jetlang",
  "ScalazActor"  = "Scalaz")

vms <- names(vm_names)

# vm_colors <- brewer.pal(length(vms_all), "Paired")  # to replace scale_fill_brewer(type = "qual", palette = "Paired")
vm_colors <- rainbow(length(vms))

names(vm_colors) <- vm_names

data <- load_data_file("data/all/136-savina-8c3f6ec5/benchmark.data")
# data <- load_all_data("data/all", "benchmark.data")

## Currently there is a bug in Graal, which causes NQueens not to compile properly
## Graal inlines to aggressively, resulting in a compilation result
## that does not fit into the code cache.
## Setting -Dgraal.TruffleMaximumRecursiveInlining=1 helps, but well, that's problematic)
data <- subset(data, Benchmark != "NQueens")

## For Savina benchmarks use Var for VM, makes subsequent handling easier
data <- transform(data, VM = ifelse(VM == "Scala", as.character(Var), as.character(VM)))
data$VM <- as.factor(data$VM)
data$Cores <- as.factor(data$Cores)

data <- subset(data, select = c(Value, Unit, Benchmark, VM, Cores, Iteration, version, sha))
data <- droplevels(subset(data, Iteration >= 100 & Iteration <= 990))

## Calculate mean for SOMns
somns <- data %>%
  filter(VM == "SOMns") %>%
  group_by(Benchmark, Cores, version, sha) %>%
  summarise(RuntimeMean = mean(Value))

norm <- data %>%
  left_join(somns) %>%
  group_by(Benchmark, Cores, version, sha) %>%
  transform(RuntimeRatio = Value / RuntimeMean)
  
# norm <- transform(data, RuntimeRatio = Value / mean(Value[VM == "Java8U66"]))
stats <- norm %>%
  group_by(VM, Benchmark, Cores, version, sha) %>%
  summarise(
    Time.ms = mean(Value),
    sd      = sd(Value),
    RuntimeFactor = mean(RuntimeRatio),
    RR.sd         = sd(RuntimeRatio),
    RR.median     = median(RuntimeRatio))

stats_cores_vm <- stats %>%
  group_by(VM, Cores, version, sha) %>%
  transform(
    VMMean = geometric.mean(RuntimeFactor),
    min = min(RuntimeFactor),
    max = max(RuntimeFactor))

stats_cores_vm_s <- stats %>%
  group_by(VM, Cores, version, sha) %>%
  summarise(
    VMMean = geometric.mean(RuntimeFactor),
    min = min(RuntimeFactor),
    max = max(RuntimeFactor))

stats_vm <- stats %>%
  group_by(VM, version, sha) %>%
  transform(
    VMMean = geometric.mean(RuntimeFactor),
    min = min(RuntimeFactor),
    max = max(RuntimeFactor))

stats_vm_s <- stats %>%
  group_by(VM, version, sha) %>%
  summarise(
    VMMean = geometric.mean(RuntimeFactor),
    min = min(RuntimeFactor),
    max = max(RuntimeFactor))


stats_latest <- stats_vm %>%
  group_by(VM) %>%
  filter(version == max(version))

plot_benchmarks_speedup_for_vms <- function(stats, vms) {
  vm_stats <- droplevels(subset(stats, VM %in% vms))

  for (b in levels(vm_stats$Benchmark)) {
    data_b <- droplevels(subset(vm_stats, Benchmark == b))

    p <- ggplot(data_b, aes(x = VM, y = RuntimeFactor, fill = VM)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = RuntimeFactor + RR.sd, ymin = RuntimeFactor - RR.sd), width=0.25) +
      coord_flip() + theme_bw() + # scale_fill_manual(values=col) +
      theme(legend.position="none") + ggtitle(b)
    tryCatch({print(p)})
  }
}

plot_benchmarks_speedup_for_vms_faceted <- function(
  stats_vm, vms, ylab = "Runtime Factor, normalized to Java\n(lower is better)") {
  vm_stats <- subset(stats_vm, VM %in% vms)
  vm_stats$VM <- revalue(vm_stats$VM, vm_names)
  vm_stats$VM <- reorder(vm_stats$VM, X=vm_stats$VMMean)
  breaks <- levels(droplevels(vm_stats)$VM)
  col_values <- sapply(breaks, function(x) vm_colors[[x]])

  p <- ggplot(vm_stats, aes(x = VM, y = RuntimeFactor, fill = VM)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = RuntimeFactor + RR.sd, ymin = RuntimeFactor - RR.sd), width=0.25) +
      facet_wrap(~ Benchmark, ncol = 1, scales="free_y") +
       theme_bw() + theme_simple(font_size = 8) + # scale_fill_manual(values=col) + coord_flip() +
      theme(legend.position="none", axis.text.x=element_text(angle=90, hjust = 1, vjust = 0.5)) +
    scale_fill_manual(values = col_values) +
    ylab(ylab)
  print(p)
}

overview_box_plot <- function(stats, vms, prepare_data = NULL, pre_plot = NULL, new_colors = FALSE) {
  # stats <- stats_latest
  # vms <- c("Node", "Pharo", "JavaInt", 
  #          "Lua53", "LuaJIT2", "SOMns-Enterprise")

  vm_stats <- stats %>%
    filter(VM %in% vms)

  vm_stats$VM <- revalue(vm_stats$VM, vm_names)
  vm_stats$VM <- reorder(vm_stats$VM, X=-vm_stats$VMMean)
  if (!is.null(prepare_data)) {
   vm_stats <- prepare_data(vm_stats)
  }
  vm_stats <- droplevels(vm_stats)

  breaks <- levels(vm_stats$VM)
  cat(breaks)
  if (new_colors) {
    col_values <- brewer.pal(length(breaks), "Paired")
  } else {
    col_values <- sapply(breaks, function(x) vm_colors[[x]])
  }

  plot <- ggplot(vm_stats, aes(x=VM, y=RuntimeFactor, fill = VM))
  if (!is.null(pre_plot)) {
    plot <- pre_plot(plot)
  }
  plot <- plot +
    geom_boxplot(outlier.size = 0.5) + #fill=get_color(5, 7)
    theme_bw() + theme_simple(font_size = 8) +
    theme(axis.text.x = element_text(angle= 90, vjust=0.5, hjust=1), legend.position="none") +
    #scale_y_log10(breaks=c(1,2,3,10,20,30,50,100,200,300,500,1000)) + #limit=c(0,30), breaks=seq(0,100,5), expand = c(0,0)
    ggtitle("Runtime Factor, normalized to Java\n(lower is better)") + coord_flip() + xlab("") +
    scale_fill_manual(values = col_values)
  plot
}
```

## Actor Performance of SOMns

**Goal**

- want to be able to claim that the SOMns actor system is on par with state-of-the-art JVM actor frameworks


- normalize a configuration to the peak performance of SOMns
- peak performance:
  - discard first 100 iterations
  - 100 as cut of determined per inspection of [warmup-plot.Rmd]
  - most configurations are stabilized at that point
  - only very few have issues afterwards, overall this should be a good
    compromise


```{r actor-overview}
# p <- ggplot(stats, aes(VM, RuntimeFactor))
# p <- p + geom_violin() + facet_wrap(~Cores, nrow=1)
# p <- p + theme_bw() + theme_simple(font_size = 8) +
#     theme(axis.text.x = element_text(angle= 90, vjust=0.5, hjust=1), legend.position="none")
# p

p <- ggplot(stats, aes(VM, RuntimeFactor))
p <- p + geom_boxplot(outlier.size = 0.5) + geom_violin(colour = "#cccccc") + facet_wrap(~Cores, nrow=1)
p <- p + theme_bw() + theme_simple(font_size = 8) +
    theme(axis.text.x = element_text(angle= 90, vjust=0.5, hjust=1), legend.position="none")
p + coord_cartesian(ylim = c(0, 5))
```


Goal:


 - compare SOMns with SOMns-enterprise
   - is enterprise faster, if not, why?

 - want to show that benchmarks benefit from parallel execution
   - for parallel benchmarks
      - RadixSort
      - UnbalancedCobwebbedTree
      - TrapezoidalApproximation
      - NQueens
 
 - identify broken benchmarks
   - incomplete runs
   - broken compilation
   - much slower than the others



All results are normalized to Java 1.8.0_91. Furthermore, we report peak
performance. This means, the reported measurements are taken after warmup and
compilation of the benchmark code is completed.

## Overview

##### Fast Language Implementations

The following set of language implementations reaches the performance of Java on
our set of benchmarks within a factor of 2 to 3 on average. To allow for a more
detailed assessment of these *fast* language implementations, we exclude slower
ones from the following plot.

```{r fast-langs-overview, fig.width=15, fig.height=6}
# overview_box_plot(stats_latest,
#                   c("Node", "Pharo", "JavaInt", "Java8U66", "MRI23", "Lua53", "LuaJIT2", "SOMns-Enterprise"), new_colors = TRUE)
p <- overview_box_plot(stats_latest, vms, pre_plot = function (p) {
  p + geom_hline(aes(yintercept=1), colour="#cccccc", linetype="dashed") +
      geom_hline(aes(yintercept=2), colour="#cccccc", linetype="dashed") +
      geom_hline(aes(yintercept=3), colour="#cccccc", linetype="dashed") })
p + scale_y_continuous(limit=c(0,13), breaks = c(1, 2, 3, 4, 6, 8, 10, 12))
```




##### Performance Overview Data
<a id="data-table"></a>

The following table contains the numerical representation of the results
depicted above.

```{r truffle-lang-table, results='asis', echo=FALSE}
vm_stats <- ddply(stats_latest, ~ VM, summarise,
                     geomean = geometric.mean(RuntimeFactor),
                     sd      = sd(RuntimeFactor),
                     min     = min(RuntimeFactor),
                     max     = max(RuntimeFactor),
                     median  = median(RuntimeFactor))
vm_stats$VM <- revalue(vm_stats$VM, vm_names)
vm_stats$VM <- reorder(vm_stats$VM, X=vm_stats$geomean)


t <- tabular(Justify("l")*Heading()*VM ~
             Heading('Runtime Factor over Java')*Justify("r")*Format(sprintf("%.2f"))*((geomean + sd + min + max + median)*Heading()*identity), data=vm_stats)
table_options(justification="c ")
html(t)
```

## Details for all Benchmarks
<a id="all-benchmarks"></a>

The following plots show results for each of the benchmarks.

##### All Benchmark Details

###### 1 Core

```{r benchmarks-c1, fig.width=4, fig.height=16}
plot_benchmarks_speedup_for_vms_faceted(subset(stats_cores_vm, Cores == 1), vms)
```

###### 2 Core

```{r benchmarks-c2, fig.width=4, fig.height=16}
plot_benchmarks_speedup_for_vms_faceted(subset(stats_cores_vm, Cores == 2), vms)
```

###### 4 Core

```{r benchmarks-c1, fig.width=4, fig.height=16}
plot_benchmarks_speedup_for_vms_faceted(subset(stats_cores_vm, Cores == 4), vms)
```

###### 6 Core

```{r benchmarks-c1, fig.width=4, fig.height=16}
plot_benchmarks_speedup_for_vms_faceted(subset(stats_cores_vm, Cores == 6), vms)
```

###### 8 Core

```{r benchmarks-c1, fig.width=4, fig.height=16}
plot_benchmarks_speedup_for_vms_faceted(subset(stats_cores_vm, Cores == 8), vms)
```
##### Slow Language Implementations

```{r slow-langs-benchmarks, fig.width=4, fig.height=16}
plot_benchmarks_speedup_for_vms_faceted(stats_latest, vms_slow)
```

##### Benchmark Results
<a id="benchmark-table"></a>

The following table contains the numerical results for all benchmarks.

```{r benchmark-table, results='asis', echo=FALSE}
t_stats <- stats_latest
t_stats$VM <- revalue(t_stats$VM, vm_names)
t_stats$VM <- reorder(t_stats$VM, X=t_stats$VMMean)

show_plain <- mean ## this is silly, but works better than the identity for missing values

t <- tabular(Justify("l")*Heading()*Benchmark*VM ~
             Heading('Runtime Factor over Java')*Justify("r")*Format(sprintf("%.2f"))*((
                 Heading("mean")*RuntimeFactor
               + Heading("sd")*RR.sd
               # + Heading("median")*RR.median
               )*Heading()*show_plain), data=t_stats)
html(t)
```
