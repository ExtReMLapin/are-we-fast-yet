# Analysis of Fork/Join Experiments

Status: 2018-01-07

- available implementations
  - SOMns-naive (naive implementation, direct mapping to f/j pool)
  - Java  (using f/j pool)

- implementations to be completed
  - SOMns-vinni (Vinni, ported from old SOMns version)
  - SOMns-TFJ   (port and cutomization of f/j pool)

- available benchmarks
 - benchmark versions
  - Nai: fork all tasks, only do a join locally
  - One: only one parallel task by forking and immediately joining
  - Opt: fork, but do one task locally
  - Seq: all fork() are compute()

Based on these benchmarks, we want to know the following things:

For each language/implementation:

- task creation overhead
  - seq vs. one
- speedup for task optimization (opt vs naive)
- parallel speedup factor (on 8 cores, 16 hyperthreads, for fix problem size)
  - opt vs seq
- parallel speedup factor (naive vs seq)
- parallel speedup factor (opt vs. one)
- parallel speedup factor (naive vs one)

Comment: that's the full set of 6 combinations computed from the data,
         all other combinations are repetitions or identity comparisons

Across languages/implementations:

Assessing performance difference:

 - nai vs. nai
 - one vs. one
 - opt vs. opt
 - seq vs. seq

```{r setup, echo=FALSE, include=FALSE}
# load libraries, the data, and prepare it
if (Sys.getenv("RSTUDIO") == "1") { setwd("/Users/smarr/Projects/PostDoc/FASTXX/are-we-fast-yet/report") }
source("scripts/libraries.R", chdir=TRUE)

load_and_install_if_necessary("knitr")

opts_chunk$set(dev = 'png',
               dev.args=list(pointsize=10),
               echo = FALSE,
               fig.keep='all',
               fig.path="figures/",
               external=FALSE,
               tidy=FALSE)
#    cache=TRUE,


load_and_install_if_necessary("yaml")

conf <- read_yaml("../../awfy-runs/codespeed.conf")

# REM: display_name is not mandatory in ReBench configs, and new/not standard, should have a fallback
vm_names <- sapply(conf$virtual_machines, function (vm) return(vm$display_name))

vms_all  <- names(vm_names)

# vm_colors <- brewer.pal(length(vms_all), "Paired")  # to replace scale_fill_brewer(type = "qual", palette = "Paired")
vm_colors <- rainbow(length(vms_all))

names(vm_colors) <- vm_names

# data <- load_all_data("data/all", "benchmark.data")
data <- load_data_file("data/all/121-450e6527/benchmark.data")
data <- droplevels(subset(data,
                          select = c(Value, Unit, Benchmark, VM, Cores, Suite, Extra, Iteration))) # , version, sha

# Filter vms_all by the data
vms_all <- intersect(vms_all, levels(data$VM))
vm_names <- vm_names[vms_all]

## We currently don't remove warmup data, there is a lot of noise anyway
## and for some benchmarks, we don't have full data, because they run out of memory/threads

# data_fast_vms      <- droplevels(subset(data, Iteration >= 500 & Iteration <= 1000 & VM %in% vms_fast))
# data_very_slow_vms <- droplevels(subset(data, VM %in% vms_slow & VM != "JRubyJ8"))
# data_slow_vms      <- droplevels(subset(data, Iteration >= 100 & (VM == "JRubyJ8" | VM == "Pharo")))
# data <- rbind(data_fast_vms, data_slow_vms, data_very_slow_vms)

#norm <- ddply(data, ~ Benchmark, transform,
#              RuntimeRatio = Value / mean(Value[VM == "Java8U66"]))

norm <- data %>%
  group_by(Benchmark, Cores) %>%
  mutate(RuntimeRatio = Value / mean(Value[VM == "GraalC2"]))
# View(subset(norm, Benchmark == "CilkSort" & Iteration > 990 & Cores == "Seq"))

stats <- norm %>%
  group_by(VM, Benchmark, Cores) %>%  # , version, sha
  summarise(
    Time.ms = mean(Value),
    sd      = sd(Value),
    RuntimeFactor = mean(RuntimeRatio),
    RR.sd         = sd(RuntimeRatio),
    RR.median     = median(RuntimeRatio))

# stats_vm <- stats %>%
#   group_by(VM, Benchmark, Cores) %>%   # , version, sha
#   mutate(
#     VMMean = geometric.mean(RuntimeFactor),
#     min = min(RuntimeFactor),
#     max = max(RuntimeFactor))

plot_benchmarks_speedup_for_vms_faceted <- function(
  stats, vms, benchmark_type, ylab = "Runtime Factor, normalized to C2\n(lower is better)") {
  # vms <- vms_all
  vm_stats <- subset(stats, VM %in% vms & Cores == benchmark_type)
  vm_stats$VM <- revalue(vm_stats$VM, vm_names)
  #vm_stats$VM <- reorder(vm_stats$VM, X=vm_stats$VMMean)

  breaks <- levels(droplevels(vm_stats)$VM)
  col_values <- sapply(breaks, function(x) vm_colors[[x]])

  p <- ggplot(vm_stats, aes(x = VM, y = RuntimeFactor, fill = VM)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = RuntimeFactor + RR.sd, ymin = RuntimeFactor - RR.sd), width=0.25) +
      facet_wrap(~ Benchmark, ncol = 1, scales="free_y") +
       theme_bw() + theme_simple(font_size = 8) + # scale_fill_manual(values=col) + coord_flip() +
      theme(legend.position="none", axis.text.x=element_text(angle=90, hjust = 1, vjust = 0.5)) +
    scale_fill_manual(values = col_values) +
    ylab(ylab) + ggtitle(benchmark_type)
  print(p)
}

overview_box_plot <- function(stats, vms, prepare_data = NULL, pre_plot = NULL, new_colors = FALSE) {
  # stats <- stats_latest
  # vms <- c("Node", "Pharo", "JavaInt", 
  #          "Lua53", "LuaJIT2", "SOMns-Enterprise")

  vm_stats <- stats %>%
    filter(VM %in% vms)

#  vm_stats$VM <- revalue(vm_stats$VM, vm_names)
#  vm_stats$VM <- reorder(vm_stats$VM, X=-vm_stats$VMMean)

  if (!is.null(prepare_data)) {
   vm_stats <- prepare_data(vm_stats)
  }
  vm_stats <- droplevels(vm_stats)

  breaks <- levels(vm_stats$VM)
  cat(breaks)
  if (new_colors) {
    col_values <- brewer.pal(length(breaks), "Paired")
  } else {
    col_values <- sapply(breaks, function(x) vm_colors[[x]])
  }

  plot <- ggplot(vm_stats, aes(x=VM, y=RuntimeFactor, fill = VM))
  if (!is.null(pre_plot)) {
    plot <- pre_plot(plot)
  }
  plot <- plot +
    geom_boxplot(outlier.size = 0.5) + #fill=get_color(5, 7)
    theme_bw() + theme_simple(font_size = 8) +
    theme(axis.text.x = element_text(angle= 90, vjust=0.5, hjust=1), legend.position="none") +
    #scale_y_log10(breaks=c(1,2,3,10,20,30,50,100,200,300,500,1000)) + #limit=c(0,30), breaks=seq(0,100,5), expand = c(0,0)
    ggtitle("Runtime Factor, normalized to Java\n(lower is better)") + coord_flip() + xlab("") +
    scale_fill_manual(values = col_values)
  plot
}
```

# Performance Overview

The results are normalized to HotSpot's C2 compiler in the same VM as the Graal Core compiler.

We see for each benchmark the result and can directly compare the performance
of the different VMs to each other, for each of the four different benchmark
implementations/types.

<style>
#perf-overview img { float: left }
#perf-overview { width: 1000px; display: block }
</style>

<span id="perf-overview">
```{r vm-perf-overview, fig.width=2, fig.height=10}
for (t in c("Seq", "One", "Nai", "Opt")) {
  # t <- "Seq"
  plot_benchmarks_speedup_for_vms_faceted(stats, vms_all, t)
  cat("\n")
}
```
</span>



# Task Creation Overhead

What's name the task creation overhead here is in fact more than simply the
creation of a task object, and scheduling it on the fork/join pool.

This overhead also includes the penalty of not being able to inline across
the spawning and spawn code, which typically results in many optimizations
not becoming applicable.

The overhead is the difference between benchmark types "Seq" and "One", i.e.,
the fully sequential version (which in Java still allocates the task object,
but doesn't spawn it) and the version that only spawns a single task at a time
and then immediately joins/waits for it.


```{r task-creation-overhead, fig.width=2, fig.height=10}
seq <- subset(data, Cores == "Seq" | Cores == "One")
seq_n <- seq %>%
  group_by(VM, Benchmark) %>%
  mutate(SlowdownFactor = Value / mean(Value[Cores == "Seq"]))

seq_s <- seq_n %>%
  group_by(VM, Benchmark, Cores) %>%  # , version, sha
  summarise(
    Time.ms = mean(Value),
    sd      = sd(Value),
    RuntimeFactor = mean(SlowdownFactor),
    RR.sd         = sd(SlowdownFactor),
    RR.median     = median(SlowdownFactor))

plot_benchmarks_speedup_for_vms_faceted(seq_s, vms_all, "One", "Runtime Factor over Sequential\n(lower is better)")
```

# Speedup for Task Optimization

A classic optimization for fork/join programs is to avoid unnecessary
task creation (and spawning) by executing at least one task locally before
waiting for the completion of other tasks.

The benefit of this optimization is calculated based on the difference between
the naive and the optimized version.

```{r}
stats_for_experiments <- function (data, slow, fast) {
  opt <- subset(data, Cores == slow | Cores == fast)
  opt_n <- opt %>%
    group_by(VM, Benchmark) %>%
    mutate(Speedup = mean(Value[Cores == fast]) / Value)

  opt_s <- opt_n %>%
    group_by(VM, Benchmark, Cores) %>%  # , version, sha
    summarise(
      Time.ms = mean(Value),
      sd      = sd(Value),
      RuntimeFactor = mean(Speedup),
      RR.sd         = sd(Speedup),
      RR.median     = median(Speedup))
  opt_s
}
```

```{r task-optimization, fig.width=2, fig.height=10}
task_opt_s <- stats_for_experiments(data, "Nai", "Opt")
plot_benchmarks_speedup_for_vms_faceted(task_opt_s, vms_all, "Nai", "Speedup over Naive\n(lower is better)")
```

# Parallel Speedup: Opt vs Seq (on 8 cores, 16 hyperthreads, for fix problem size)

```{r opt-vs-seq, fig.width=2, fig.height=10}
opt_s <- stats_for_experiments(data, "Seq", "Opt")
plot_benchmarks_speedup_for_vms_faceted(opt_s, vms_all, "Seq", "Speedup over Sequential\n(lower is better)")
```

# Parallel Speedup: Naive vs Seq

```{r nai-vs-seq, fig.width=2, fig.height=10}
opt_s <- stats_for_experiments(data, "Seq", "Nai")
plot_benchmarks_speedup_for_vms_faceted(opt_s, vms_all, "Seq", "Speedup over Sequential\n(lower is better)")
```

# Parallel Speedup: Opt vs One

```{r opt-vs-one, fig.width=2, fig.height=10}
opt_s <- stats_for_experiments(data, "One", "Opt")
plot_benchmarks_speedup_for_vms_faceted(opt_s, vms_all, "One", "Speedup over Single Task\n(lower is better)")
```

# Parallel Speedup: Naive vs One

```{r nai-vs-one, fig.width=2, fig.height=10}
opt_s <- stats_for_experiments(data, "One", "Nai")
plot_benchmarks_speedup_for_vms_faceted(opt_s, vms_all, "One", "Speedup over Single Task\n(lower is better)")
```
