# Analysis of Fork/Join Experiments

Status: 2018-01-13

- available implementations
  - SOMns-naive (naive implementation, direct mapping to f/j pool)
  - Java  (using f/j pool)

- implementations to be completed
  - SOMns-vinni (Vinni, ported from old SOMns version)
  - SOMns-TFJ   (port and cutomization of f/j pool)

- available benchmarks
 - benchmark versions
  - Nai: fork all tasks, only do a join locally
  - One: only one parallel task by forking and immediately joining
  - Opt: fork, but do one task locally
  - Seq: all fork() are compute()
 - most run with smaller and bigger problem sizes to see whether we get parallel speedups

Based on these benchmarks, we want to know the following things:

For each language/implementation:

- task creation overhead
  - seq vs. one
- speedup for task optimization (opt vs naive)
- parallel speedup factor (on 8 cores, 16 hyperthreads, for fix problem size)
  - opt vs seq
- parallel speedup factor (naive vs seq)
- parallel speedup factor (opt vs. one)
- parallel speedup factor (naive vs one)

Comment: that's the full set of 6 combinations computed from the data,
         all other combinations are repetitions or identity comparisons

Across languages/implementations:

Assessing performance difference:

 - nai vs. nai
 - one vs. one
 - opt vs. opt
 - seq vs. seq

```{r setup, echo=FALSE, include=FALSE}
# load libraries, the data, and prepare it
if (Sys.getenv("RSTUDIO") == "1") { setwd("/Users/smarr/Projects/PostDoc/FASTXX/are-we-fast-yet/report") }
source("scripts/libraries.R", chdir=TRUE)

load_and_install_if_necessary("knitr")

opts_chunk$set(dev = 'svg', #dev = 'png', 
               dev.args=list(pointsize=10),
               echo = FALSE,
               fig.keep='all',
               fig.path="figures/",
               external=FALSE,
               tidy=FALSE)
#    cache=TRUE,


load_and_install_if_necessary("yaml")

conf <- read_yaml("../../awfy-runs/codespeed.conf")

# REM: display_name is not mandatory in ReBench configs, and new/not standard, should have a fallback
vm_names <- sapply(conf$virtual_machines, function (vm) return(vm$display_name))

vms_all  <- names(vm_names)

# vm_colors <- brewer.pal(length(vms_all), "Paired")  # to replace scale_fill_brewer(type = "qual", palette = "Paired")
vm_colors <- rainbow(length(vms_all))

names(vm_colors) <- vm_names

# data <- load_all_data("data/all", "benchmark.data")
data <- load_data_file("data/all/121-450e6527/benchmark.data")
data <- rbind(data, load_data_file("data/all/123-1f03c713/benchmark.data"))
data <- rbind(data, load_data_file("data/all/125-a38dc89d/benchmark.data"))
data <- droplevels(subset(data,
                          select = c(Value, Unit, Benchmark, VM, Cores, Suite, Extra, Iteration))) # , version, sha

# Filter vms_all by the data
vms_all <- intersect(vms_all, levels(data$VM))
vm_names <- vm_names[vms_all]

## We currently don't remove warmup data, there is a lot of noise anyway
## and for some benchmarks, we don't have full data, because they run out of memory/threads

# data_fast_vms      <- droplevels(subset(data, Iteration >= 500 & Iteration <= 1000 & VM %in% vms_fast))
# data_very_slow_vms <- droplevels(subset(data, VM %in% vms_slow & VM != "JRubyJ8"))
# data_slow_vms      <- droplevels(subset(data, Iteration >= 100 & (VM == "JRubyJ8" | VM == "Pharo")))
# data <- rbind(data_fast_vms, data_slow_vms, data_very_slow_vms)

#norm <- ddply(data, ~ Benchmark, transform,
#              RuntimeRatio = Value / mean(Value[VM == "Java8U66"]))

baseline_vm <- "GraalC2"

norm <- data %>%
  group_by(Benchmark, Cores, Extra) %>%
  mutate(RuntimeRatio = Value / mean(Value[VM == baseline_vm]),
         Base = mean(Value[VM == baseline_vm]))
# View(subset(norm, Benchmark == "CilkSort" & Iteration > 990 & Cores == "Seq"))

stats <- norm %>%
  group_by(VM, Benchmark, Cores, Extra) %>%  # , version, sha
  summarise(
    Time.ms = mean(Value),
    sd      = sd(Value),
    RuntimeFactor = mean(RuntimeRatio),
    RR.sd         = sd(RuntimeRatio),
    RR.median     = median(RuntimeRatio),
    Base = mean(Base))

# stats_vm <- stats %>%
#   group_by(VM, Benchmark, Cores) %>%   # , version, sha
#   mutate(
#     VMMean = geometric.mean(RuntimeFactor),
#     min = min(RuntimeFactor),
#     max = max(RuntimeFactor))

plot_benchmarks_speedup_for_vms_faceted <- function(
  stats, vms, benchmark_type, ylab = "Runtime Factor, normalized to C2\n(lower is better)") {
  # benchmark_type <- "Nai"
  # vms <- c("GraalBasic", "GraalC2")
  # vms <- vms_all
  vm_stats <- subset(stats, VM %in% vms & Cores == benchmark_type)
  vm_stats$VM <- revalue(vm_stats$VM, vm_names)
  vm_stats <- droplevels(vm_stats)
  #vm_stats$VM <- reorder(vm_stats$VM, X=vm_stats$VMMean)

  breaks <- levels(droplevels(vm_stats)$VM)
  col_values <- sapply(breaks, function(x) vm_colors[[x]])

  p <- ggplot(vm_stats, aes(x = VM, y = RuntimeFactor, fill = VM)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = RuntimeFactor + RR.sd, ymin = RuntimeFactor - RR.sd), width=0.25) +
      facet_wrap(~ Benchmark + Extra, ncol = 1, scales="free_y") +
       theme_bw() + theme_simple(font_size = 8) + # scale_fill_manual(values=col) + coord_flip() +
      theme(legend.position="none", axis.text.x=element_text(angle=90, hjust = 1, vjust = 0.5)) +
    scale_fill_manual(values = col_values) +
    ylab(ylab) + ggtitle(benchmark_type)
  print(p)
}

overview_box_plot <- function(stats, vms, prepare_data = NULL, pre_plot = NULL, new_colors = FALSE) {
  # vms <- c("GraalBasic", "GraalC2")

  vm_stats <- stats %>%
    filter(VM %in% vms)

#  vm_stats$VM <- revalue(vm_stats$VM, vm_names)
#  vm_stats$VM <- reorder(vm_stats$VM, X=-vm_stats$VMMean)

  if (!is.null(prepare_data)) {
   vm_stats <- prepare_data(vm_stats)
  }
  vm_stats <- droplevels(vm_stats)

  breaks <- levels(vm_stats$VM)
  
  if (new_colors) {
    col_values <- brewer.pal(length(breaks), "Paired")
  } else {
    col_values <- sapply(breaks, function(x) vm_colors[[x]])
  }

  plot <- ggplot(vm_stats, aes(x=VM, y=RuntimeFactor, fill = VM))
  if (!is.null(pre_plot)) {
    plot <- pre_plot(plot)
  }
  plot <- plot +
    geom_boxplot(outlier.size = 0.5) + #fill=get_color(5, 7)
    theme_bw() + theme_simple(font_size = 8) +
    theme(axis.text.x = element_text(angle= 90, vjust=0.5, hjust=1), legend.position="none") +
    #scale_y_log10(breaks=c(1,2,3,10,20,30,50,100,200,300,500,1000)) + #limit=c(0,30), breaks=seq(0,100,5), expand = c(0,0)
    ggtitle("Runtime Factor, normalized to Java\n(lower is better)") + coord_flip() + xlab("") +
    scale_fill_manual(values = col_values)
  plot
}

stats_for_base_vs_opt <- function (data_for_stats, base_t, opt_t) {
  opt <- subset(data_for_stats, Cores == base_t | Cores == opt_t)
  opt_n <- opt %>%
    group_by(VM, Benchmark, Extra) %>%
    mutate(RuntimeRatio = Value / mean(Value[Cores == base_t]),
           Base         = mean(Value[Cores == base_t]))

  opt_s <- opt_n %>%
    group_by(VM, Benchmark, Cores, Extra) %>%  # , version, sha
    summarise(
      Time.ms = mean(Value),
      sd      = sd(Value),
      RuntimeFactor = mean(RuntimeRatio),
      RR.sd         = sd(RuntimeRatio),
      RR.median     = median(RuntimeRatio),
      Base          = mean(Base))
  opt_s
}

stats_table <- function (opt_s, benchmark_type, runtime_factor_title = "Runtime Factor over Sequential\n(lower is better)") {
  filtered_s <- filter(opt_s, Cores == benchmark_type)
  
  show_plain <- mean ## this is silly, but works better than the identity for missing values
  
  t <- tabular(Justify("l")*Heading()*Benchmark*VM*Heading("Size")*Extra ~
               Heading(runtime_factor_title, character.only=TRUE)*Justify("r")*Format(sprintf("%.2f"))*((
                   Heading("mean")*RuntimeFactor
                 + Heading("sd")*RR.sd
                 + Heading("median")*RR.median
                 + Heading("time (ms)")*Time.ms
                 + Heading("base (ms)")*Base
                 )*Heading()*show_plain), data=filtered_s)
  html(t[!is.na(t[,2])])
}
```

# Performance Overview

The results are normalized to HotSpot's C2 compiler in the same VM as the Graal Core compiler.

We see for each benchmark the result and can directly compare the performance
of the different VMs to each other, for each of the four different benchmark
implementations/types.

<style>
#perf-overview img { float: left }
#perf-overview { width: 1000px; display: block }
</style>

<span id="perf-overview">
```{r vm-perf-overview, fig.width=2, fig.height=15}
for (t in c("Seq", "One", "Nai", "Opt")) {
  # t <- "Seq"
  plot_benchmarks_speedup_for_vms_faceted(stats, vms_all, t)
  cat("\n")
}
```
</span>

```{r vm-perf-overview-table, results='asis', echo=FALSE}
for (benchmark_type in c("Seq", "One", "Nai", "Opt")) {
  t_stats <-  subset(stats, Cores == benchmark_type)
  t_stats$VM <- revalue(t_stats$VM, vm_names)
  #t_stats$VM <- reorder(t_stats$VM, X=t_stats$VMMean)
  
  cat(paste0("<h3>", benchmark_type, "</h3>"))
  
  stats_table(t_stats, benchmark_type, paste0('Runtime Factor over ', vm_names[baseline_vm]))
}
```


# Task Creation Overhead

What's name the task creation overhead here is in fact more than simply the
creation of a task object, and scheduling it on the fork/join pool.

This overhead also includes the penalty of not being able to inline across
the spawning and spawn code, which typically results in many optimizations
not becoming applicable.

The overhead is the difference between benchmark types "Seq" and "One", i.e.,
the fully sequential version (which in Java still allocates the task object,
but doesn't spawn it) and the version that only spawns a single task at a time
and then immediately joins/waits for it.


```{r task-creation-overhead, fig.width=2, fig.height=10}
seq_s <- stats_for_base_vs_opt(data, "Seq", "One")
plot_benchmarks_speedup_for_vms_faceted(seq_s, vms_all, "One", "Runtime Factor over Sequential\n(lower is better)")
```

```{r task-creation-overhead-table, results='asis', echo=FALSE}
stats_table(seq_s, "One", "Runtime Factor over Sequential\n(lower is better)")
```

# Speedup for Task Optimization

A classic optimization for fork/join programs is to avoid unnecessary
task creation (and spawning) by executing at least one task locally before
waiting for the completion of other tasks.

The benefit of this optimization is calculated based on the difference between
the naive and the optimized version.

```{r task-optimization, fig.width=2, fig.height=10}
task_opt_s <- stats_for_base_vs_opt(data, "Nai", "Opt")
plot_benchmarks_speedup_for_vms_faceted(task_opt_s, vms_all, "Opt", "Runtime Factor over Naive\n(lower is better)")
```

```{r task-optimization-table, results='asis', echo=FALSE}
stats_table(task_opt_s, "Opt", "Runtime Factor over Naive\n(lower is better)")
```

# Parallel Speedup: Opt vs Seq (on 8 cores, 16 hyperthreads, for fix problem size)

```{r opt-vs-seq, fig.width=2, fig.height=10}
opt_s <- stats_for_base_vs_opt(data, "Seq", "Opt")
plot_benchmarks_speedup_for_vms_faceted(opt_s, vms_all, "Opt", "Runtime Factor over Sequential\n(lower is better)")
```

```{r opt-vs-seq-table, results='asis', echo=FALSE}
stats_table(opt_s, "Opt", "Runtime Factor over Sequential\n(lower is better)")
```

# Parallel Speedup: Naive vs Seq

```{r nai-vs-seq, fig.width=2, fig.height=10}
opt_s <- stats_for_base_vs_opt(data, "Seq", "Nai")
plot_benchmarks_speedup_for_vms_faceted(opt_s, vms_all, "Nai", "Runtime Factor over Sequential\n(lower is better)")
```

```{r nai-vs-seq-table, results='asis', echo=FALSE}
stats_table(opt_s, "Nai", "Runtime Factor over Sequential\n(lower is better)")
```

# Parallel Speedup: Opt vs One

```{r opt-vs-one, fig.width=2, fig.height=10}
opt_s <- stats_for_base_vs_opt(data, "One", "Opt")
plot_benchmarks_speedup_for_vms_faceted(opt_s, vms_all, "Opt", "Runtime Factor over Single Task\n(lower is better)")
```

```{r opt-vs-one-table, results='asis', echo=FALSE}
stats_table(opt_s, "Opt", "Runtime Factor over Single Task\n(lower is better)")
```

# Parallel Speedup: Naive vs One

```{r nai-vs-one, fig.width=2, fig.height=10}
opt_s <- stats_for_base_vs_opt(data, "One", "Nai")
plot_benchmarks_speedup_for_vms_faceted(opt_s, vms_all, "Nai", "Runtime Factor over Single Task\n(lower is better)")
```

```{r nai-vs-one-table, results='asis', echo=FALSE}
stats_table(opt_s, "Nai", "Runtime Factor over Sequential\n(lower is better)")
```
