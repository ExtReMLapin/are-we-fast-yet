# -*- mode: yaml -*-
# Config file for ReBench
standard_experiment: all
standard_data_file: 'are-we-there-yet.data'

# settings and requirements for statistic evaluation
statistics:
    confidence_level: 0.95

runs:
    number_of_data_points: 100

# settings for quick runs, useful for fast feedback during experiments
quick_runs:
    number_of_data_points: 3
    max_time: 60   # time in seconds

# definition of benchmark suites
benchmark_suites:
    macro-steady:
        performance_reader: LogPerformance
        command: " -cp Smalltalk:Examples/Benchmarks/Richards:Examples/Benchmarks/DeltaBlue Examples/Benchmarks/BenchmarkHarness.som  %(benchmark)s "
        max_runtime: 60000
        benchmarks:
            - Richards:
                extra_args: "105 0 100"
                warmup: 5
            - DeltaBlue:
                extra_args: "140 0 10000"
                warmup: 40
            - Mandelbrot:
                extra_args: "140 0 1000"
                warmup: 40
    macro-startup:
        performance_reader: LogPerformance
        command: " -cp Smalltalk:Examples/Benchmarks/Richards:Examples/Benchmarks/DeltaBlue Examples/Benchmarks/BenchmarkHarness.som  %(benchmark)s "
        max_runtime: 60000
        benchmarks:
            - Richards:
                extra_args: "1 0 100"
            - DeltaBlue:
                extra_args: "1 0 10000"
            - Mandelbrot:
                extra_args: "1 0 1000"

    macro-steady-java:
        performance_reader: LogPerformance
        location: Classic-Benchmarks
        command: " %(benchmark)s "
        max_runtime: 600
        benchmarks:
            - Richards:
                extra_args: "104 0 100"
                warmup: 4
            - DeltaBlue:
                extra_args: "103 0 10000"
                warmup: 3
            - Mandelbrot:
                extra_args: "1000 110"
                warmup: 10

    macro-steady-pypy:
        performance_reader: LogPerformance
        location: Classic-Benchmarks
        command: " %(benchmark)s "
        max_runtime: 600
        benchmarks:
            - Richards:
                command:    richards.py
                extra_args: "8 0 100"
                warmup: 3
            - DeltaBlue:
                command:    deltablue.py
                extra_args: "103 0 10000"
                warmup: 3
            - Mandelbrot:
                command:    mandelbrot.py
                extra_args: "1000 110"
                warmup: 10

    # macro-steady-cogvm:
    #     performance_reader: LogPerformance
    #     location: .
    #     command: " Pharo-bench.image SMarkHarness %(benchmark)s 1 1 "
    #     max_runtime: 600
    #     benchmarks:
    #         - SMarkRichards.benchRichards:
    #             extra_args: 100
    #         - SMarkDeltaBlue.benchDeltaBlue:
    #             extra_args: 10000


# VMs have a name and are specified by a path and the binary to be executed
virtual_machines:
    Java:
        path: /usr/bin/
        binary: java
        args: -server
    # Java-Int:
    #     path: /usr/bin/
    #     binary: java
    #     args: -Xint
    PyPy:
        path: /usr/bin/
        binary: env
        args: "pypy"
    # TruffleSOM-interpreter:
    #     path: TruffleSOM
    #     binary: som.sh
    #     args: ""
    TruffleSOM-graal:
        path: TruffleSOM
        binary: ../graal/mxtool/mx
        args: " --vm server vm -G:-TraceTruffleInlining -G:-TraceTruffleCompilation -Xbootclasspath/a:build/classes:../graal/truffle.jar som.vm.Universe"
    TruffleSOM-graal-no-split:
        path: TruffleSOM
        binary: ../graal/mxtool/mx
        args: " --vm server vm -G:-TruffleSplittingEnabled -G:-TraceTruffleInlining -G:-TraceTruffleCompilation -Xbootclasspath/a:build/classes:../graal/truffle.jar som.vm.Universe"

    # RPySOM-interpreter:
    #     path: .
    #     binary: RPySOM-no-jit
    #     args: "-cp Smalltalk"
    # RPySOM-non-recursive-jit:
    #     path: RPySOM-non-recursive
    #     binary: RPySOM-jit
    #     args: ""
    RPySOM-recursive-jit:
        path: RPySOM-recursive
        binary: RPySOM-jit
        args: ""
    SOMpp:
        path: SOMpp
        binary: som.sh
        args: ""
    # CogVM:
    #     path: .
    #     binary: pharo.sh

# define the benchmarks to be executed for a re-executable benchmark run
experiments:
    Java:
        actions:    benchmark
        benchmark:  macro-steady-java
        executions:
            - Java
    # LuaJIT:
    #     actions:    benchmark
    #     benchmark:  macro-steady-lua
    #     executions: LuaJIT
    # CogVM:
    #     actions: benchmark
    #     benchmark: macro-steady-cogvm
    #     executions: CogVM
    PyPy:
        actions:    benchmark
        benchmark:  macro-steady-pypy
        executions: PyPy
    SOM:
        description: All benchmarks on SOM
        actions: benchmark
        benchmark:
            - macro-steady
        executions:
            - TruffleSOM-graal
            - TruffleSOM-graal-no-split
            - RPySOM-recursive-jit
    SOMpp:
        benchmark:
            - macro-startup
        executions:
            - SOMpp
