Performance Evaluation of RPySOM and TruffleSOM
===============================================

The main question for our paper _[Are we there yet? Simple 
Language-Implementation Techniques for the 21st Century][1]_ was whether
these simple AST ([TruffleSOM]) or bytecode ([RPySOM]) interpreter
implementations based on [Truffle] and [RPython] could reach performance of
the same order of magnitude as for instance Java on top
of a highly optimizing JVM.

In order to answer this question, we chose DeltaBlue and Richards as
object-oriented benchmarks and Mandelbrot as a numerical
one. All three are classic benchmarks that have been used to tune for instance
JVMs, JavaScript VMs, PyPy, as well as Smalltalk VMs. Still, they are rather
specific benchmarks that can given an indicate of what the expected
performance of a VM is, but _they are not predictors_ for the performance
of concrete applications.

Methodology
-----------

The benchmarking methodology is kept as simple as possible. For each benchmark,
we roughly determined when the VM reaches stable state and then executed it
100 times to account for non-deterministic influences such as caches and
garbage collection. Each benchmark executes with a predefined _problem size_
on each VM and the results are compared directly with each other. 

The used benchmark machine has two quad-core Intel Xeons E5520, 2.26 GHz with
8 GB of memory and runs Ubuntu Linux with kernel 3.11. For a few more details
of the machine, see the [specification](data/spec.html).

```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide', errors=FALSE}
# load libraries, the data, and prepare it
if (Sys.getenv("RSTUDIO") == "1") { setwd("/Users/smarr/Projects/PostDoc/ARE-WE-THERE-YETXX/perf-eval-repo/") }

options(scipen=5)

source("scripts/libraries.R", chdir=TRUE)
data <- load_data_file("data/are-we-there-yet.data")
data <- subset(data, select = c(Value, Unit, Benchmark, VM, Suite))
data <- prepare_vm_names(data)
macro <- droplevels(subset(data, grepl("macro", Suite)))
micro <- droplevels(subset(data, grepl("micro", Suite)))
```

Overall Performance
-------------------

```{r echo=FALSE}
# aggregate results for display
stats <- ddply(macro, ~ Benchmark + VM + Suite,
               summarise,
               Time.mean                 = mean(Value),
               Time.geomean              = geometric.mean(Value),
               Time.stddev               = sd(Value),
               Time.median               = median(Value),
               max = max(Value),
               min = min(Value))

# normalize for each benchmark separately to the Java baseline
norm <- ddply(stats, ~ Benchmark, transform,
              RuntimeRatio = Time.mean / Time.mean[VM == "Java"])

sompp_runtime_ratio <- subset(norm, VM == "SOM++",      select = c(RuntimeRatio))
rpy_runtime_ratio   <- subset(norm, VM == "RPySOM",     select = c(RuntimeRatio))
tru_runtime_ratio   <- subset(norm, VM == "TruffleSOM", select = c(RuntimeRatio))
```

RPySOM and TruffleSOM reach more or less the goal, i.e., the same order
of magnitude of performance as the Java Virtual Machine. RPySOM reaches a
runtime of a factor
`r round(min(rpy_runtime_ratio), 1)` to `r round(max(rpy_runtime_ratio), 1)`, 
while TruffleSOM has more remaining optimization potential with being a factor
`r round(min(tru_runtime_ratio), 1)` to `r round(max(tru_runtime_ratio), 1)`
slower, while being faster than RPySOM on two of the three benchmarks.
This performance is reached without requiring custom VMs and hundreds
of person years of engineering. Thus, we conclude, that RPython as well as
Truffle live up to the expectations. 

```{r echo=FALSE, fig.width=4.3, fig.height=2.8, dev='svg'}
# summarize to VMs
vms <- ddply(norm, ~ VM,
             summarise,
             RunRatio.geomean = geometric.mean(RuntimeRatio))

# create a simple bar chart
plot <- ggplot(subset(vms, VM != "SOM++" & VM != "TruffleSOM.ns"), aes_string(x="VM", y="RunRatio.geomean"))
plot <- plot + geom_bar(stat="identity",
                   colour=get_color(5, 6),
                   size=.3,        # Thinner lines
                   fill=get_color(5, 7),
                   width=0.75) + scale_y_continuous(limit=c(0,5), 
                                                    breaks=seq(0,100,1), 
                                                    expand = c(0,0)) +
  ylab("Runtime, normalized to Java\nlower is better")

plot <- plot + theme_simple()
plot
```

A look at the details for the three benchmarks shows that the performance
varies widely depending on the benchmark, which indicates further optimization
potential for specific code characteristics.

```{r echo=FALSE, fig.width=6.8, fig.height=4.5, dev='svg'}
# normalize for each benchmark separately to the Java baseline
benchs <- ddply(stats, ~ Benchmark, transform,
                RuntimeRatio = Time.mean / Time.mean[VM == "Java"])
benchs <- subset(benchs, VM != "SOM++" & VM != "TruffleSOM.ns")

# create a simple bar chart
plot <- ggplot(benchs, aes_string(x="VM", y="RuntimeRatio"))
plot <- plot + geom_bar(stat="identity",
                   colour=get_color(5, 6),
                   size=.3,        # Thinner lines
                   fill=get_color(5, 7),
                   width=0.75) + scale_y_continuous(limit=c(0,18),
                                                    breaks=seq(0,100,2),
                                                    expand = c(0,0)) +
  ylab("Runtime, normalized to Java (lower is better)")
plot <- plot + facet_wrap(~ Benchmark)
plot <- plot + theme_simple() +
    theme(axis.text.x = element_text(angle= 90, vjust=0.5, hjust=1))
plot
```

As a further reference point, we did measurements also for the SOM++
interpreter, a SOM implemented in C++. It uses bytecodes and applies
optimizations such as inline caching, threaded interpretation, and a
generational garbage collector. However, it is 
`r round(min(sompp_runtime_ratio), 0)` to `r round(max(sompp_runtime_ratio), 0)`x slower than Java.
Since it is significantly slower, we did not include it in the charts because
it would distort the impression. It is however included in the table below.

```{r echo=FALSE, results='asis'}
# benchs <- ddply(subset(stats, VM != "TruffleSOM.ns"), ~ Benchmark, transform,
#                RuntimeRatio = round(Time.mean / Time.mean[VM == "Java"], digits = 1),
#                Time.stddev = round(Time.stddev, digits = 1),
#                Time.mean = round(Time.mean, digits = 0))
# benchs <- benchs[c("Benchmark", "VM", "Time.mean", "Time.stddev", "RuntimeRatio", "Time.geomean", "Time.median", "min", "max")]
# kable(benchs, format = "markdown", digits=2)
# kable(benchs, format = "html", digits=1)

writeLines("<div class='full center'>")
html(tabular(Justify(l,data=l)*Benchmark ~ Format(digits=2)*Heading('Runtime in ms')*VM*Heading()*Value*Justify(data=r)*(mean + sd),  data=droplevels(subset(macro,VM != "TruffleSOM.ns"))))
writeLines("</div>")
```

Microbenchmarks
---------------

In addition to DeltaBlue, Richards, and Mandelbrot, which can be considered
macro or kernel benchmarks, we also use a number of more focused
microbenchmarks to assess the performance of the SOM implementations.
Since these are specific to SOM, we do not have numbers for the other
languages.

```{r echo=FALSE, fig.width=6, fig.height=4, dev="svg"}
micro <- subset(micro, VM != "TruffleSOM")
levels(micro$VM)  <- map_names(levels(micro$VM), list("TruffleSOM.ns" = "TruffleSOM"))
stats <- ddply(micro, ~ Benchmark + VM + Suite,
               summarise,
               Time.mean                 = mean(Value),
               Time.geomean              = geometric.mean(Value),
               Time.stddev               = sd(Value),
               Time.median               = median(Value),
               max = max(Value),
               min = min(Value))

norm <- ddply(stats, ~ Benchmark, transform,
              RuntimeRatio = Time.mean / Time.mean[VM == "RPySOM"])


plot <- ggplot(norm, aes(x=Benchmark, y=RuntimeRatio, fill=VM))
plot <- plot + geom_bar(stat="identity",
                   size=.3,        # Thinner lines
                   position=position_dodge(.9),
                   width=0.75) + coord_flip() +
  xlab("") + ggtitle("Runtime normalized to RPySOM (lower values are better)")
plot <- plot +
    scale_fill_manual(values=c(get_color(5, 7), get_color(3, 7))) +
    scale_y_continuous(breaks=seq(0,100, 0.1), expand = c(0,0), limits = c(0, 1.2)) +
    theme_simple() +
    theme(legend.position      = c(0.9,0.9),
          legend.background    = element_blank())
plot
```


```{r 'microbenchmarks-table', echo=FALSE, results='asis'}
#kable(stats, format = "html", digits=2)
writeLines("<div class='half center'>")
html(tabular(Justify(l,data=l)*Benchmark ~ Format(digits=2)*Heading('Runtime in ms')*VM*Heading()*Value*Justify(data=r)*(mean + sd),  data=micro))
writeLines("</div>")
```

[1]:          http://stefan-marr.de/research/
[RPySOM]:     https://github.com/SOM-st/RPySOM
[TruffleSOM]: https://github.com/SOM-st/TruffleSOM
[Truffle]:    https://wiki.openjdk.java.net/display/Graal/Truffle+FAQ+and+Guidelines
[RPython]:    http://pypy.readthedocs.org/en/latest/translation.html