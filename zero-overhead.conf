# -*- mode: yaml -*-
# Config file for ReBench
standard_experiment: all
standard_data_file: 'zero-overhead.data'

# settings and requirements for statistic evaluation
statistics:
    confidence_level: 0.95

runs:
    number_of_data_points: 100

# settings for quick runs, useful for fast feedback during experiments
quick_runs:
    number_of_data_points: 3
    max_time: 60   # time in seconds

# definition of benchmark suites
benchmark_suites:
    macro-steady:
        performance_reader: LogPerformance
        command: &MACRO_COMMAND " -cp Smalltalk:Examples/Benchmarks/Richards:Examples/Benchmarks/DeltaBlue Examples/Benchmarks/BenchmarkHarness.som  %(benchmark)s "
        max_runtime: 60000
        benchmarks:
            - Richards:
                extra_args: "400 0 100"
                warmup: 0
            - DeltaBlue:
                extra_args: "400 0 10000"
                warmup: 0
            - Mandelbrot:
                extra_args: "400 0 1000"
                warmup: 0
            - NBody:
                extra_args: "400 0 250000"
                warmup: 0
    macro-startup:
        performance_reader: LogPerformance
        command: *MACRO_COMMAND
        max_runtime: 60000
        benchmarks:
            - Richards:
                extra_args: "1 0 100"
            - DeltaBlue:
                extra_args: "1 0 10000"
            - Mandelbrot:
                extra_args: "1 0 1000"
            - NBody:
                extra_args: "1 0 250000"

    # macro-steady-java:
    #     performance_reader: LogPerformance
    #     location: implementations/classic-benchmarks/
    #     command: " %(benchmark)s "
    #     max_runtime: 600
    #     benchmarks:
    #         - Richards:
    #             extra_args: "104 0 100"
    #             warmup: 4
    #         - DeltaBlue:
    #             extra_args: "103 0 10000"
    #             warmup: 3
    #         - Mandelbrot:
    #             extra_args: "1000 110"
    #             warmup: 10

    # macro-steady-pypy:
    #     performance_reader: LogPerformance
    #     location: implementations/classic-benchmarks/benchmarks/
    #     command: " %(benchmark)s "
    #     max_runtime: 600
    #     benchmarks:
    #         - Richards:
    #             command:    richards.py
    #             extra_args: "110 0 100"
    #             warmup: 10
    #         - DeltaBlue:
    #             command:    deltablue.py
    #             extra_args: "103 0 10000"
    #             warmup: 3
    #         - Mandelbrot:
    #             command:    mandelbrot.py
    #             extra_args: "1000 110"
    #             warmup: 10

    micro-steady:
        performance_reader: LogPerformance
        command: " -cp Smalltalk Examples/Benchmarks/BenchmarkHarness.som %(benchmark)s "
        max_runtime: 60000
        benchmarks:
            - Bounce:
                extra_args: "110 0 200"
                warmup: 10
            - BubbleSort:
                extra_args: "115 0 300"
                warmup: 15
            - Dispatch:
                extra_args: "110 0 2000"
                warmup: 10
            - Fannkuch:
                extra_args: "110 0 8"
                warmup: 10
            - Fibonacci:
                extra_args: "110 0 300"
                warmup: 10
            - FieldLoop:
                extra_args: "110 0 300"
                warmup: 10
            - IntegerLoop:
                extra_args: "110 0 800"
                warmup: 10
            - List:
                extra_args: "115 0 200"
                warmup: 15
            - Loop:
                extra_args: "110 0 1000"
                warmup: 10
            - Permute:
                extra_args: "110 0 300"
                warmup: 10
            - Queens:
                extra_args: "125 0 200"
                warmup: 25
            - QuickSort:
                extra_args: "110 0 300"
                warmup: 10
            - Recurse:
                extra_args: "110 0 300"
                warmup: 10
            - Sieve:
                extra_args: "110 0 500"
                warmup: 10
            - Storage:
                extra_args: "115 0 200"
                warmup: 15
            - Sum:
                extra_args: "110 0 1000"
                warmup: 10
            - Towers:
                extra_args: "110 0 200"
                warmup: 10
            - TreeSort:
                extra_args: "120 0 100"
                warmup: 20
            - WhileLoop:
                extra_args: "110 0 3000"
                warmup: 10

# VMs have a name and are specified by a path and the binary to be executed
virtual_machines:
    Java:
        path: /usr/bin/
        binary: java
        args: -server
    # Java-Int:
    #     path: /usr/bin/
    #     binary: java
    #     args: -Xint
    PyPy:
        path: /usr/bin/
        binary: env
        args: "pypy"
    # TruffleSOM-interpreter:
    #     path: TruffleSOM
    #     binary: som.sh
    #     args: ""
    TruffleSOM-graal:
        path: implementations/TruffleSOM
        binary: ../graal.sh
        args: " -G:-TraceTruffleInlining -G:-TraceTruffleCompilation -Xbootclasspath/a:build/classes:../graal/truffle.jar som.vm.Universe"
    TruffleSOM-graal-no-split:
        path: implementations/TruffleSOM
        binary: ../graal.sh
        args: " -G:-TruffleSplittingEnabled -G:-TraceTruffleInlining -G:-TraceTruffleCompilation -Xbootclasspath/a:build/classes:../graal/truffle.jar som.vm.Universe"

    # RPySOM-interpreter:
    #     path: .
    #     binary: RPySOM-no-jit
    #     args: "-cp Smalltalk"
    # RPySOM-non-recursive-jit:
    #     path: RPySOM-non-recursive
    #     binary: RPySOM-jit
    #     args: ""
    RPySOM-jit:
        path: implementations/RPySOM
        binary: RPySOM-jit
        args: ""
    SOMpp:
        path: implementations/SOMpp
        binary: som.sh
        args: ""
    # CogVM:
    #     path: .
    #     binary: pharo.sh

# define the benchmarks to be executed for a re-executable benchmark run
experiments:
    # Java:
    #     actions:    benchmark
    #     benchmark:  macro-steady-java
    #     executions:
    #         - Java
    # LuaJIT:
    #     actions:    benchmark
    #     benchmark:  macro-steady-lua
    #     executions: LuaJIT
    # PyPy:
    #     actions:    benchmark
    #     benchmark:  macro-steady-pypy
    #     executions: PyPy
    SOM:
        description: All benchmarks on SOM
        actions: benchmark
        benchmark:
            - macro-steady
            - micro-steady
        executions:
            - TruffleSOM-graal
            - TruffleSOM-graal-no-split
            - RPySOM-jit
