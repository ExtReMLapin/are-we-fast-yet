Inherent Performance Overhead of the OMOP
=========================================

TODO: back to main


```{r init, echo=FALSE, message=FALSE, warning=FALSE, results='hide', errors=FALSE}
# load libraries, the data, and prepare it
if (Sys.getenv("RSTUDIO") == "1") { setwd("/Users/smarr/Projects/PostDoc/ZERO-MOP/perf-eval-selfopt/") }

options(scipen=5)

source("scripts/libraries.R", chdir=TRUE)
data <- load_data_file("data/zero-overhead.data")
data <- subset(data, select = c(Value, Unit, Benchmark, VM, Suite, Var, rid))
data <- prepare_vm_names(data)

## Add a time series id
data <- ddply(data, ~ Benchmark + VM + Var, transform,
              Iteration = rid - min(rid))

warmup_plot <- function(bench) {
  upperBound <- 2 * median(bench$Value)
  b <- ddply(bench, ~ Benchmark + Var + VM, here(transform),
             ValCut = pmin(Value, upperBound))
  
  # b$Foo <- as.factor(paste0(as.character(b$VM), '-', as.character(b$Var)))
  #plot <- ggplot(b, aes(x=Iteration, y=ValCut)) + geom_line(aes(colour = Var)) #+ facet_grid(VM + Benchmark ~ .)
  plot <- ggplot(b, aes(x=Iteration, y=ValCut)) + geom_line(aes(colour = interaction(Var, VM))) #+ facet_grid(VM + Benchmark ~ .)
  plot <- plot + ggtitle(paste0(bench_name, ' - ', vm))
    #plot <- plot + scale_y_continuous(limits = c(0, 3000))
  print(plot)
}
```

Identifying Usable Sample Ranges
--------------------------------

To account for the VM warmup, first we need to identify useable ranges of
measurements for each benchmark. We want to exclude compilation effects and
only consider the peak performance. Thus, we inspect all available
measurements of a benchmark as a time series based on the iteration within
the same VM invocation. Based on the observed behavior, we select a continous
range of measurements that could represent the peak performance and does not
exhibit obvious indication for ongoing compilation.

In order to make the graphs readable, we chose an upper cutoff value of two
times the median over the whole set of samples.


```{r complete-overview, fig.width=12, fig.height=4, dev='svg'}
omop <- droplevels(subset(data, Suite == "micro-steady-omop" | Suite == "macro-steady-omop"))

for (suite in c("micro-steady-omop", "macro-steady-omop")) {
  benchmarks <- levels(droplevels(subset(omop, Suite == suite))$Benchmark)
  vms        <- levels(droplevels(subset(omop, Suite == suite))$VM)

  for (bench_name in benchmarks) {
    for (vm in vms) {
      warmup_plot(droplevels(subset(data,
                    Suite == suite & VM == vm & Benchmark == bench_name)))
    }
  }
}
```

Selected Warmed-up Sample Ranges
--------------------------------

For the benchmarks in the group micro-steady-omop, we will select the interval
[210,350] iterations as a stable range. This range seems to be the best overall
range without having to cut for each benchmark separately.

For the macro-steady-omop benchmarks, we select [600,990].

```{r selection-overview, fig.width=12, fig.height=4, dev='svg'}
micro <- droplevels(subset(data, 
            (Suite == "micro-steady-omop" & Iteration >= 210 & Iteration <= 350 & Benchmark != "TreeSort") |
            (Suite == "micro-steady-omop" & Iteration >= 210 + 200 & Iteration <= 350 + 200 & Benchmark == "TreeSort")))

for (bench_name in levels(micro$Benchmark)) {
  for (vm in levels(micro$VM)) {
    # warmup_plot(droplevels(subset(micro, VM == vm & Benchmark == bench_name)))
  }
}

macro <- droplevels(subset(data, Suite == "macro-steady-omop" & Iteration >= 600 & Iteration <= 990))

for (bench_name in levels(macro$Benchmark)) {
  for (vm in levels(macro$VM)) {
    # warmup_plot(droplevels(subset(macro, VM == vm & Benchmark == bench_name)))
  }
}
```

Selecting the Graal Configuration with best results
---------------------------------------------------

Graal and Truffle are still not completely mature when it comes to the
inlining and specialization behavior that is used to optimize ASTs. Thus, we
run the experiments with older behavior and two variants of recently added
behavior which seem to have a significant impact on peak performance.

For our purposes, we select the variant that has best average performance.

```{r selection-graal, results='asis'}
omop <- rbind(micro, macro)

stats <- ddply(omop, ~ Benchmark + VM + Suite + Var, summarise,
               Time.geomean = geometric.mean(Value))
norm  <- ddply(stats, ~ Benchmark + VM + Suite, transform,
               RuntimeRatio = Time.geomean / Time.geomean[Var == "false"])
norm  <- subset(norm, Var == "true", c(Benchmark, VM, RuntimeRatio, Time.geomean))

norm_vm <- ddply(norm, ~ VM, summarise,
                 VM.geomean = geometric.mean(RuntimeRatio))
kable(norm_vm)
```

So, as we can see from the above table, there is some difference between the
different approaches to inlining AST trees. Since the old approach provides
the best performance in our case, we will select it for the final evaluation.


```{r benchmark-overview, fig.width=8, fig.height=6, dev='svg'}
## Boxplot with all benchmarks, only the enforced version, normalized, and shown around 1.0 line
norm_omop <- ddply(subset(omop, VM == "TruffleSOM.os (OMOP)" | VM == "RTruffleSOM (OMOP)"),
                   ~ Benchmark + VM + Suite, transform,
                   RuntimeRatio = Value / geometric.mean(Value[Var == "false"]))
norm_omop_enforced <- droplevels(subset(norm_omop, Var == "true"))

vms <- levels(norm_omop_enforced$VM)

for (vm in vms) {
  p <- ggplot(droplevels(subset(norm_omop_enforced, VM==vm)), aes(x=Benchmark, y=RuntimeRatio))
  p <- p + ggtitle(vm) + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
  print(p + geom_boxplot())
}
```


stats <- ddply(omop, ~ Benchmark + VM + Suite + Var,
               summarise,
               Time.geomean              = geometric.mean(Value),
               Time.stddev               = sd(Value),
               Time.median               = median(Value),
               max = max(Value),
               min = min(Value))
norm <- ddply(stats, ~ Benchmark + VM + Suite, transform,
              RuntimeRatio = Time.geomean / Time.geomean[Var == "false"])

norm <- subset(norm, Var == "true", c(Benchmark, VM, RuntimeRatio, Time.geomean, Time.stddev, Time.median, max, min))
norm

slow <- subset(norm, RuntimeRatio > 1.5)
slow


#norm_vm <- ddply(subset(norm, Benchmark != "Sieve" & Benchmark != "Queens"), ~ VM,
norm_vm <- ddply(norm, ~ VM,
                 summarise,
                 VM.geomean = geometric.mean(RuntimeRatio),
                 min = min(RuntimeRatio),
                 max = max(RuntimeRatio))
norm_vm



benchmarks <- levels(omop$Benchmark)

for (vm in levels(omop$VM)) {
  for (bench in benchmarks) {
    r <- subset(omop, VM == vm & Benchmark == bench)
    r <- droplevels(r)
    
    p <- ggplot(r, aes(x=Var, y=Value))
    p <- p + ggtitle(vm)
    print(p + geom_boxplot())
  }
}

vms <- subset(levels(omop$VM), !grepl(".002", levels(omop$VM)))

for (bench_name in levels(omop$Benchmark)) {
for (vm in vms) {
  #bench_name = "Bounce"
  b <- droplevels(subset(omop, Benchmark == bench_name & (VM == vm | VM == paste0(vm, '.002'))))
  
  upperBound <- 2 * median(b$Value)
  b <- ddply(b, ~ Benchmark + Var + VM, transform,
             ValCut = pmin(Value, upperBound))
  # b$Foo <- as.factor(paste0(as.character(b$VM), '-', as.character(b$Var)))
    
  #plot <- ggplot(b, aes(x=Iteration, y=ValCut)) + geom_line(aes(colour = Var)) #+ facet_grid(VM + Benchmark ~ .)
  plot <- ggplot(b, aes(x=Iteration, y=ValCut)) + geom_line(aes(colour = interaction(Var, VM))) #+ facet_grid(VM + Benchmark ~ .)
  plot <- plot + ggtitle(paste0(bench_name, ' - ', vm))
    #plot <- plot + scale_y_continuous(limits = c(0, 3000))
  print(plot)
    
    # file_name = paste0("cut-timeline-", bench_name, ".pdf")
    # cat(file_name)
    # ggsave(plot, file=file_name)
}}


stats <- ddply(subset(omop, Iteration > 200), ~ Benchmark + VM + Suite + Var,
               summarise,
               Time.geomean              = geometric.mean(Value),
               Time.stddev               = sd(Value),
               Time.median               = median(Value),
               max = max(Value),
               min = min(Value))
norm <- ddply(stats, ~ Benchmark + VM + Suite, transform,
              RuntimeRatio = Time.geomean / Time.geomean[Var == "false"])

norm <- subset(norm, Var == "true", c(Benchmark, VM, RuntimeRatio, Time.geomean, Time.stddev, Time.median, max, min))
norm

slow <- subset(norm, RuntimeRatio > 1.5)
slow


norm_vm <- ddply(subset(norm, Benchmark != "Sieve"), ~ VM,
                 summarise,
                 VM.geomean = geometric.mean(RuntimeRatio),
                 min = min(RuntimeRatio),
                 max = max(RuntimeRatio))
norm_vm

## Boxplot with all benchmarks, only the enforced version, normalized, and shown around 1.0 line
norm_omop <- ddply(omop, ~ Benchmark + VM + Suite, transform,
                   RuntimeRatio = Value / geometric.mean(Value[Var == "false"]))
norm_omop_enforced <- droplevels(subset(norm_omop, Var == "true"))

vms <- levels(norm_omop_enforced$VM)

for (vm in vms) {
  p <- ggplot(droplevels(subset(norm_omop_enforced, VM==vm)), aes(x=Benchmark, y=RuntimeRatio))
  p <- p + ggtitle(vm)
  print(p + geom_boxplot())
}





[1]:          http://stefan-marr.de/research/
[RPySOM]:     https://github.com/SOM-st/RPySOM
[TruffleSOM]: https://github.com/SOM-st/TruffleSOM
[Truffle]:    https://wiki.openjdk.java.net/display/Graal/Truffle+FAQ+and+Guidelines
[RPython]:    http://pypy.readthedocs.org/en/latest/translation.html